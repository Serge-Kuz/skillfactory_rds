{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попробуем реализовать stcking  \n",
    "## Создадим классы-обертки для xgboost, lightgbm, catboost -- таким образом обеспечим единообразие обработки. В качестве мета-алгоритма примем xgboost.  \n",
    "\n",
    "# В итоге, STACKING НЕ ДАЛ УЛУЧШЕНИЯ результата!!!! Возможно, не подобрал гипер-параметры, особено lightgbm ((   \n",
    "# НО, скорее всего, мало данных всего 9000 записей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: lightgbm in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.0)WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\nYou should consider upgrading via the 'c:\\programdata\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n\nRequirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.4.1)\nRequirement already satisfied: scikit-learn!=0.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (0.23.1)\nRequirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.19.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.16.0)\n"
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "import sklearn\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sys\n",
    "import os\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import ast\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.models import LdaModel, LdaMulticore, Phrases\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "import re\n",
    "import logging\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Python       : 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\nNumpy        : 1.19.1\n"
    }
   ],
   "source": [
    "print('Python       :', sys.version.split('\\n')[0])\n",
    "print('Numpy        :', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING: Could not generate requirement for distribution -ip 20.2.1 (c:\\programdata\\anaconda3\\lib\\site-packages): Parse error at \"'-ip==20.'\": Expected W:(abcd...)\nWARNING: Could not generate requirement for distribution - p (c:\\programdata\\anaconda3\\lib\\site-packages): Parse error at \"'-===p'\": Expected W:(abcd...)\nWARNING: Could not generate requirement for distribution -andas 0.25.1 (c:\\programdata\\anaconda3\\lib\\site-packages): Parse error at \"'-andas=='\": Expected W:(abcd...)\n"
    }
   ],
   "source": [
    "# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION    = 35\n",
    "DIR_TRAIN  = './input/04-09-2020/' # подключил к ноутбуку свой внешний датасет\n",
    "DIR_TEST   = './input/sf-dst-car-price/'\n",
    "VAL_SIZE   = 0.1 #0.33   # 33%\n",
    "N_FOLDS    = 5\n",
    "\n",
    "CURR_YEAR = 2020\n",
    "PRICE_IN_TIME = 0.89 #0.9 # За время соревнования (7- мес) цены на подержанные авто выросли\n",
    "ROUND_C = int(5000)\n",
    "# CATBOOST\n",
    "ITERATIONS = 6000#4000\n",
    "LR         = 0.1 #0.05# 0.05#0.1#0.05#0.1\n",
    "OD_PVAL = 0.002 #0.002 \n",
    "L2_REG = 4.5 #4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = pd.read_csv('C:/Users/YBNY3VWM/SF/SkillFactory/MODELS-CARS/v2/X_sub_all.csv')\n",
    "X = pd.read_csv('C:/Users/YBNY3VWM/SF/SkillFactory/MODELS-CARS/v2/X_all.csv')\n",
    "y = pd.read_csv('C:/Users/YBNY3VWM/SF/SkillFactory/MODELS-CARS/v2/y_all.csv')\n",
    "cat_features = [] \n",
    "with open('C:/Users/YBNY3VWM/SF/SkillFactory/MODELS-CARS/v3-подрезаны фичи!/cat_features.npy', 'rb') as f:\n",
    "    cat_features = list(np.load(f))\n",
    "\n",
    "y.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "X.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "X_sub.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "y=y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_mape(preds, dtrain):\n",
    "   labels = dtrain.get_label()\n",
    "   return('mape', np.mean(np.abs((labels - preds) / (labels + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_mape(preds, dtrain):\n",
    "   labels = dtrain.get_label()\n",
    "   return('mape', np.mean(np.abs((labels - preds) / (labels + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_pred-y_true)/y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, ntrain, ntest, kf, train, labels, test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((5, ntest))\n",
    "\n",
    "    print('--->',kf.n_splits, )\n",
    "    N_FOLDS=5\n",
    "    splits = list(KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED).split(X, y))\n",
    "    for i, (train_index, test_index) in tqdm(enumerate(splits), total=N_FOLDS,):\n",
    "        print('i --->', i)\n",
    "        x_tr = train.iloc[train_index]\n",
    "        y_tr = labels.iloc[train_index]\n",
    "        x_te = train.iloc[test_index]\n",
    "        y_te = labels.iloc[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr, x_te, y_te)\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        prd=clf.predict(test)\n",
    "        oof_test_skf[i, :] = prd\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=RANDOM_SEED, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 400)\n",
    "\n",
    "    def train(self, xtra, ytra, xte, yte):\n",
    "        \n",
    "        dtrain = xgb.DMatrix(xtra, label=ytra)\n",
    "        dvalid = xgb.DMatrix(xte, label=yte)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds,\n",
    "            watchlist, feval=xgb_mape, early_stopping_rounds=10 )\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LgbWrapper(object):\n",
    "    def __init__(self, seed=RANDOM_SEED, cat_features='auto', params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.param['categorical_feature'] = cat_features\n",
    "        self.nrounds = params.pop('nrounds', 400)\n",
    "        self.cat_features = cat_features\n",
    "\n",
    "    def train(self, xtra, ytra, xte, yte):\n",
    "\n",
    "        ytra = ytra.values.ravel()\n",
    "        yte = yte.values.ravel()\n",
    "        dtrain = lgb.Dataset(xtra, label=ytra)\n",
    "        self.gbdt = lgb.train(self.param, dtrain, self.nrounds,  feval=xgb_mape)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtbWrapper(object):\n",
    "    def __init__(self, seed=RANDOM_SEED, cat_features=None, params=None):\n",
    "        self.seed = seed\n",
    "        self.nrounds = 300\n",
    "        self.cat_features = cat_features\n",
    "\n",
    "    def train(self, xtra, ytra, xte, yte):\n",
    "        self.gbdt = ctb.CatBoostRegressor(iterations = ITERATIONS,\n",
    "                          learning_rate = LR,\n",
    "                          random_seed = RANDOM_SEED,\n",
    "                          eval_metric='MAPE',\n",
    "                          od_type = 'IncToDec',\n",
    "                          od_pval = OD_PVAL,\n",
    "                        #   #one_hot_max_size=10,\n",
    "                        #   #random_strength=3,\n",
    "                        #   l2_leaf_reg =L2_REG\n",
    "                          )\n",
    "\n",
    "        xtra = pd.DataFrame(xtra)\n",
    "        ytra = pd.DataFrame(ytra)\n",
    "        xte = pd.DataFrame(xte)\n",
    "        yte = pd.DataFrame(yte)\n",
    "\n",
    "        self.gbdt.fit(X=xtra, y=ytra, cat_features=self.cat_features,\n",
    "                      eval_set=(xte, yte),\n",
    "                      verbose_eval=100,\n",
    "                      use_best_model=True)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(train, labels, test, cat_features_in):\n",
    "\n",
    "    ntrain = train.shape[0]\n",
    "    ntest = test.shape[0]\n",
    "\n",
    "    kf = KFold(ntrain, n_splits=5,\n",
    "               shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "    cg = CtbWrapper(seed=RANDOM_SEED, cat_features=cat_features_in, params=ctb_params)\n",
    "    xg = XgbWrapper(seed=RANDOM_SEED, params=xgb_params)\n",
    "    lg = LgbWrapper(seed=RANDOM_SEED, cat_features=cat_features_in, params=lgb_params)\n",
    "    \n",
    "    Xd = train.drop(['name', 'vehicleConfiguration'], axis = 1) \n",
    "    X_sub_d = test.drop(['name', 'vehicleConfiguration'], axis = 1) \n",
    "    \n",
    "    Xd = pd.get_dummies(Xd, drop_first=True)\n",
    "    X_sub_d = pd.get_dummies(X_sub_d, drop_first=True)\n",
    "    cols = list(set(Xd.columns) & set(X_sub_d.columns))\n",
    "    Xd = Xd[cols]\n",
    "    X_sub_d  = X_sub_d [cols]\n",
    "\n",
    "\n",
    "    cg_oof_train, cg_oof_test = get_oof(cg, ntrain, ntest, kf, train, labels, test)\n",
    "    lg_oof_train, lg_oof_test = get_oof(lg, ntrain, ntest, kf, Xd, labels, X_sub_d)    \n",
    "    xg_oof_train, xg_oof_test = get_oof(xg, ntrain, ntest, kf, Xd, labels, X_sub_d)\n",
    "  \n",
    "\n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "        \n",
    "    print(\"CG-CV: {}\".format(mean_absolute_percentage_error(labels, cg_oof_train.ravel())))\n",
    "    print(\"XG-CV: {}\".format(mean_absolute_percentage_error(labels, xg_oof_train.ravel())))\n",
    "    print(\"LG-CV: {}\".format(mean_absolute_percentage_error(labels, lg_oof_train.ravel())))\n",
    "\n",
    "\n",
    "    x_train = np.concatenate((cg_oof_train.reshape(-1,1), xg_oof_train.reshape(-1,1), lg_oof_train.reshape(-1,1)),axis=1)\n",
    "    print('------>', np.shape(x_train))\n",
    "\n",
    "    x_test = np.concatenate((cg_oof_test.reshape(-1,1), xg_oof_test.reshape(-1,1), lg_oof_test.reshape(-1,1)), axis=1)\n",
    "    print('=============>', np.shape(x_test))\n",
    "\n",
    "    np.save(arr=x_train, file='x_concat_train.npy')\n",
    "    np.save(arr=x_test, file='x_concat_test.npy')\n",
    "    np.save(arr=labels, file='y_labels.npy')\n",
    "\n",
    "    return x_train, labels, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2():\n",
    "    train = np.load('x_concat_train.npy')\n",
    "    labels = np.load('y_labels.npy')\n",
    "    test = np.load('x_concat_test.npy')\n",
    "\n",
    "    dtrain = xgb.DMatrix(train, label=labels)\n",
    "    dtest = xgb.DMatrix(test)\n",
    "\n",
    "    xgb_params = {}\n",
    "    xgb_params[\"objective\"] = \"reg:squarederror\"\n",
    "    xgb_params[\"eta\"] = 0.01\n",
    "    xgb_params[\"subsample\"] = 0.9\n",
    "    xgb_params[\"silent\"] = 1\n",
    "    xgb_params[\"max_depth\"] = 5\n",
    "    xgb_params['eval_metric'] = 'rmse'\n",
    "    xgb_params['min_child_weight'] = 10\n",
    "    xgb_params['seed'] = RANDOM_SEED\n",
    "\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=500, nfold=5, seed=RANDOM_SEED, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=10, show_stdv=True, feval=xgb_mape)\n",
    "    \n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    cv_mean = res.iloc[-1, 0]\n",
    "    cv_std = res.iloc[-1, 1]\n",
    "\n",
    "    print('')\n",
    "    print(f'Ensemble-CV: {cv_mean}+/-{cv_std}')\n",
    "    print('1----')\n",
    "    bst = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    print('2----')\n",
    "    \n",
    "    preds = bst.predict(dtest)\n",
    "    print('3----')\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = pd.read_csv('C:/Users/YBNY3VWM/SF/SkillFactory/MODELS-CARS/v3-подрезаны фичи!/X_sub_all.csv')\n",
    "X = pd.read_csv('C:/Users/YBNY3VWM/SF/SkillFactory/MODELS-CARS/v3-подрезаны фичи!/X_all.csv')\n",
    "y = pd.read_csv('C:/Users/YBNY3VWM/SF/SkillFactory/MODELS-CARS/v3-подрезаны фичи!/y_all.csv')\n",
    "cat_features = [] \n",
    "with open('C:/Users/YBNY3VWM/SF/SkillFactory/MODELS-CARS/v3-подрезаны фичи!/cat_features.npy', 'rb') as f:\n",
    "    cat_features = list(np.load(f))\n",
    "\n",
    "\n",
    "\n",
    "y.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "X.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "X_sub.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "X['Комплектация_1'].fillna('-NaN-', inplace=True)\n",
    "X_sub['Комплектация_1'].fillna('-NaN-', inplace=True)\n",
    "y=y.values.ravel()\n",
    "y = pd.Series(y).astype('float')\n",
    "cat_features = [X.columns.get_loc(c) for c in cat_features if c in X]\n",
    "from random import randrange\n",
    "import re\n",
    "X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', 'x_'+str(randrange(1900))+'_'+str(randrange(100)) , x))\n",
    "columns = X.columns\n",
    "X_sub.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l2',\n",
    "    'metric': 'mape',\n",
    "    'num_leaves': 96,\n",
    "    'max_depth': 12,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'lambda_l1': 2,\n",
    "    'lambda_l2': 42,\n",
    "    'n_estimators' : 100,\n",
    "    'max_depth' : -1, \n",
    "    #'early_stopping_round':20\n",
    " }\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'disable_default_eval_metric' : 1,\n",
    "    #'eval_metric': 'mape',\n",
    "    'learning_rate' : 0.1,\n",
    "    'base_score': 0.5,\n",
    "    'booster': 'gbtree',\n",
    "    'colsample_bylevel': 1,\n",
    "    'colsample_bynode': 1,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'gamma': 0,\n",
    "    'gpu_id': -1,\n",
    "    'importance_type': 'gain',\n",
    "    'interaction_constraints': '',\n",
    "    'max_delta_step': 0,\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 1,\n",
    "    'missing': np.nan,\n",
    "    'monotone_constraints': '()',\n",
    "    'n_estimators': 60,\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 1,\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'reg_alpha': 10,\n",
    "    'reg_lambda': 3,\n",
    "    'scale_pos_weight': 1,\n",
    "    'subsample': 0.7,\n",
    "    'tree_method': 'exact',\n",
    "    'validate_parameters': 1,\n",
    "    'verbosity': None,\n",
    "    'alpha': 10\n",
    "    }\n",
    "\n",
    "ctb_params = {\n",
    "    'iterations' : 4000,\n",
    "    'learning_rate' : 0.05,\n",
    "    'eval_metric' : 'MAPE',\n",
    "    'random_seed' : RANDOM_SEED,\n",
    "    'od_type' : 'IncToDec',\n",
    "    'od_pval' : OD_PVAL,\n",
    "    'l2_leaf_reg' : L2_REG\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---> 9809\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94dc689edc374ffda99aaa6fc6ffb089"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "i ---> 0\n0:\tlearn: 1.1927966\ttest: 1.1361373\tbest: 1.1361373 (0)\ttotal: 125ms\tremaining: 12m 30s\n100:\tlearn: 0.1605712\ttest: 0.1618782\tbest: 0.1618782 (100)\ttotal: 11.5s\tremaining: 11m 13s\n200:\tlearn: 0.1400566\ttest: 0.1473202\tbest: 0.1473202 (200)\ttotal: 22.2s\tremaining: 10m 40s\n300:\tlearn: 0.1298397\ttest: 0.1420379\tbest: 0.1418769 (298)\ttotal: 33.9s\tremaining: 10m 41s\n400:\tlearn: 0.1226315\ttest: 0.1394884\tbest: 0.1394369 (396)\ttotal: 45s\tremaining: 10m 28s\n500:\tlearn: 0.1177089\ttest: 0.1376663\tbest: 0.1376024 (494)\ttotal: 56.5s\tremaining: 10m 20s\n600:\tlearn: 0.1128698\ttest: 0.1360119\tbest: 0.1360119 (600)\ttotal: 1m 7s\tremaining: 10m 8s\n700:\tlearn: 0.1089805\ttest: 0.1352213\tbest: 0.1351757 (694)\ttotal: 1m 19s\tremaining: 9m 57s\n800:\tlearn: 0.1056581\ttest: 0.1345567\tbest: 0.1344599 (793)\ttotal: 1m 30s\tremaining: 9m 46s\n900:\tlearn: 0.1026367\ttest: 0.1341862\tbest: 0.1341697 (899)\ttotal: 1m 42s\tremaining: 9m 38s\n1000:\tlearn: 0.0991465\ttest: 0.1337912\tbest: 0.1337757 (993)\ttotal: 1m 53s\tremaining: 9m 27s\n1100:\tlearn: 0.0966678\ttest: 0.1335034\tbest: 0.1334260 (1092)\ttotal: 2m 5s\tremaining: 9m 16s\n1200:\tlearn: 0.0938578\ttest: 0.1333627\tbest: 0.1333081 (1198)\ttotal: 2m 16s\tremaining: 9m 5s\n1300:\tlearn: 0.0917037\ttest: 0.1333849\tbest: 0.1333081 (1198)\ttotal: 2m 27s\tremaining: 8m 53s\n1400:\tlearn: 0.0893348\ttest: 0.1333880\tbest: 0.1333081 (1198)\ttotal: 2m 39s\tremaining: 8m 41s\n1500:\tlearn: 0.0871617\ttest: 0.1339645\tbest: 0.1333081 (1198)\ttotal: 2m 50s\tremaining: 8m 31s\nStopped by overfitting detector  (20 iterations wait)\n\nbestTest = 0.1333080867\nbestIteration = 1198\n\nShrink model to first 1199 iterations.\ni ---> 1\n0:\tlearn: 1.1895917\ttest: 1.2083695\tbest: 1.2083695 (0)\ttotal: 134ms\tremaining: 13m 23s\n100:\tlearn: 0.1693647\ttest: 0.1707525\tbest: 0.1705994 (97)\ttotal: 11.4s\tremaining: 11m 3s\n200:\tlearn: 0.1428977\ttest: 0.1494712\tbest: 0.1494712 (200)\ttotal: 22.3s\tremaining: 10m 42s\n300:\tlearn: 0.1320464\ttest: 0.1433565\tbest: 0.1433565 (300)\ttotal: 34.1s\tremaining: 10m 45s\n400:\tlearn: 0.1245951\ttest: 0.1401143\tbest: 0.1400227 (399)\ttotal: 45.4s\tremaining: 10m 34s\n500:\tlearn: 0.1187521\ttest: 0.1380662\tbest: 0.1380662 (500)\ttotal: 56.6s\tremaining: 10m 21s\n600:\tlearn: 0.1142962\ttest: 0.1369628\tbest: 0.1369117 (595)\ttotal: 1m 7s\tremaining: 10m 9s\n700:\tlearn: 0.1098060\ttest: 0.1361057\tbest: 0.1361057 (700)\ttotal: 1m 18s\tremaining: 9m 56s\n800:\tlearn: 0.1062878\ttest: 0.1360712\tbest: 0.1356795 (727)\ttotal: 1m 30s\tremaining: 9m 49s\n900:\tlearn: 0.1024308\ttest: 0.1355403\tbest: 0.1353862 (893)\ttotal: 1m 42s\tremaining: 9m 41s\n1000:\tlearn: 0.0994351\ttest: 0.1352581\tbest: 0.1352516 (975)\ttotal: 1m 54s\tremaining: 9m 31s\n1100:\tlearn: 0.0965484\ttest: 0.1350908\tbest: 0.1349723 (1053)\ttotal: 2m 6s\tremaining: 9m 22s\n1200:\tlearn: 0.0939335\ttest: 0.1352612\tbest: 0.1349723 (1053)\ttotal: 2m 18s\tremaining: 9m 13s\n1300:\tlearn: 0.0916726\ttest: 0.1352391\tbest: 0.1349723 (1053)\ttotal: 2m 30s\tremaining: 9m 3s\n1400:\tlearn: 0.0890045\ttest: 0.1347824\tbest: 0.1347393 (1376)\ttotal: 2m 42s\tremaining: 8m 52s\n1500:\tlearn: 0.0870810\ttest: 0.1349014\tbest: 0.1347393 (1376)\ttotal: 2m 53s\tremaining: 8m 41s\n1600:\tlearn: 0.0846951\ttest: 0.1348902\tbest: 0.1346662 (1539)\ttotal: 3m 5s\tremaining: 8m 29s\n1700:\tlearn: 0.0822689\ttest: 0.1349991\tbest: 0.1346662 (1539)\ttotal: 3m 16s\tremaining: 8m 17s\n1800:\tlearn: 0.0805771\ttest: 0.1351283\tbest: 0.1346662 (1539)\ttotal: 3m 27s\tremaining: 8m 4s\n1900:\tlearn: 0.0786572\ttest: 0.1349486\tbest: 0.1346662 (1539)\ttotal: 3m 39s\tremaining: 7m 53s\n2000:\tlearn: 0.0771895\ttest: 0.1348766\tbest: 0.1346662 (1539)\ttotal: 3m 51s\tremaining: 7m 41s\n2100:\tlearn: 0.0753791\ttest: 0.1349664\tbest: 0.1346662 (1539)\ttotal: 4m 2s\tremaining: 7m 30s\n2200:\tlearn: 0.0740850\ttest: 0.1349084\tbest: 0.1346662 (1539)\ttotal: 4m 14s\tremaining: 7m 18s\n2300:\tlearn: 0.0724027\ttest: 0.1350869\tbest: 0.1346662 (1539)\ttotal: 4m 25s\tremaining: 7m 7s\n2400:\tlearn: 0.0707314\ttest: 0.1351298\tbest: 0.1346662 (1539)\ttotal: 4m 37s\tremaining: 6m 55s\n2500:\tlearn: 0.0694946\ttest: 0.1350553\tbest: 0.1346662 (1539)\ttotal: 4m 49s\tremaining: 6m 44s\nStopped by overfitting detector  (20 iterations wait)\n\nbestTest = 0.1346661957\nbestIteration = 1539\n\nShrink model to first 1540 iterations.\ni ---> 2\n0:\tlearn: 1.1956948\ttest: 1.1956765\tbest: 1.1956765 (0)\ttotal: 120ms\tremaining: 12m 2s\n100:\tlearn: 0.1677755\ttest: 0.1750033\tbest: 0.1750033 (100)\ttotal: 11.6s\tremaining: 11m 17s\n200:\tlearn: 0.1461033\ttest: 0.1582666\tbest: 0.1580297 (195)\ttotal: 22.8s\tremaining: 10m 58s\n300:\tlearn: 0.1338424\ttest: 0.1523060\tbest: 0.1523060 (300)\ttotal: 34.8s\tremaining: 10m 59s\n400:\tlearn: 0.1255152\ttest: 0.1477160\tbest: 0.1476676 (399)\ttotal: 46.5s\tremaining: 10m 49s\n500:\tlearn: 0.1200124\ttest: 0.1452588\tbest: 0.1451902 (490)\ttotal: 57.2s\tremaining: 10m 27s\n600:\tlearn: 0.1148570\ttest: 0.1438771\tbest: 0.1438604 (599)\ttotal: 1m 8s\tremaining: 10m 16s\n700:\tlearn: 0.1105681\ttest: 0.1434187\tbest: 0.1433658 (698)\ttotal: 1m 19s\tremaining: 9m 58s\n800:\tlearn: 0.1068594\ttest: 0.1430873\tbest: 0.1428723 (778)\ttotal: 1m 29s\tremaining: 9m 44s\n900:\tlearn: 0.1036009\ttest: 0.1426761\tbest: 0.1426614 (899)\ttotal: 1m 40s\tremaining: 9m 31s\n1000:\tlearn: 0.1005600\ttest: 0.1427097\tbest: 0.1425442 (957)\ttotal: 1m 51s\tremaining: 9m 17s\n1100:\tlearn: 0.0976842\ttest: 0.1425571\tbest: 0.1424579 (1067)\ttotal: 2m 2s\tremaining: 9m 6s\n1200:\tlearn: 0.0949148\ttest: 0.1425780\tbest: 0.1423519 (1166)\ttotal: 2m 13s\tremaining: 8m 53s\n1300:\tlearn: 0.0925682\ttest: 0.1423274\tbest: 0.1421656 (1288)\ttotal: 2m 24s\tremaining: 8m 40s\n1400:\tlearn: 0.0900133\ttest: 0.1428184\tbest: 0.1421656 (1288)\ttotal: 2m 35s\tremaining: 8m 31s\nStopped by overfitting detector  (20 iterations wait)\n\nbestTest = 0.1421656439\nbestIteration = 1288\n\nShrink model to first 1289 iterations.\ni ---> 3\n0:\tlearn: 1.2043372\ttest: 1.1983242\tbest: 1.1983242 (0)\ttotal: 136ms\tremaining: 13m 35s\n100:\tlearn: 0.1700794\ttest: 0.1765102\tbest: 0.1765102 (100)\ttotal: 10.7s\tremaining: 10m 25s\n200:\tlearn: 0.1427862\ttest: 0.1545137\tbest: 0.1545137 (200)\ttotal: 21.2s\tremaining: 10m 12s\n300:\tlearn: 0.1300489\ttest: 0.1461821\tbest: 0.1460541 (298)\ttotal: 31.8s\tremaining: 10m 2s\n400:\tlearn: 0.1231012\ttest: 0.1426709\tbest: 0.1426709 (400)\ttotal: 42.1s\tremaining: 9m 47s\n500:\tlearn: 0.1175562\ttest: 0.1403061\tbest: 0.1403061 (500)\ttotal: 52.3s\tremaining: 9m 33s\n600:\tlearn: 0.1131871\ttest: 0.1390295\tbest: 0.1390131 (598)\ttotal: 1m 2s\tremaining: 9m 22s\n700:\tlearn: 0.1090573\ttest: 0.1375780\tbest: 0.1374721 (690)\ttotal: 1m 13s\tremaining: 9m 12s\n800:\tlearn: 0.1054500\ttest: 0.1367568\tbest: 0.1367568 (800)\ttotal: 1m 23s\tremaining: 9m 1s\n900:\tlearn: 0.1024417\ttest: 0.1363224\tbest: 0.1363083 (898)\ttotal: 1m 34s\tremaining: 8m 53s\n1000:\tlearn: 0.0994710\ttest: 0.1358340\tbest: 0.1358340 (1000)\ttotal: 1m 44s\tremaining: 8m 43s\n1100:\tlearn: 0.0966061\ttest: 0.1355049\tbest: 0.1354897 (1099)\ttotal: 1m 55s\tremaining: 8m 33s\n1200:\tlearn: 0.0938498\ttest: 0.1349919\tbest: 0.1349900 (1190)\ttotal: 2m 5s\tremaining: 8m 22s\n1300:\tlearn: 0.0914223\ttest: 0.1352483\tbest: 0.1349293 (1235)\ttotal: 2m 16s\tremaining: 8m 13s\n1400:\tlearn: 0.0887764\ttest: 0.1348061\tbest: 0.1347780 (1389)\ttotal: 2m 26s\tremaining: 8m 1s\n1500:\tlearn: 0.0866112\ttest: 0.1344471\tbest: 0.1344427 (1495)\ttotal: 2m 37s\tremaining: 7m 52s\n1600:\tlearn: 0.0845331\ttest: 0.1347369\tbest: 0.1344427 (1495)\ttotal: 2m 48s\tremaining: 7m 41s\n1700:\tlearn: 0.0824738\ttest: 0.1347886\tbest: 0.1344427 (1495)\ttotal: 2m 58s\tremaining: 7m 31s\n1800:\tlearn: 0.0805160\ttest: 0.1348761\tbest: 0.1344427 (1495)\ttotal: 3m 10s\tremaining: 7m 23s\n1900:\tlearn: 0.0783265\ttest: 0.1348080\tbest: 0.1344427 (1495)\ttotal: 3m 20s\tremaining: 7m 12s\n2000:\tlearn: 0.0762821\ttest: 0.1350324\tbest: 0.1344427 (1495)\ttotal: 3m 31s\tremaining: 7m 2s\nStopped by overfitting detector  (20 iterations wait)\n\nbestTest = 0.1344426649\nbestIteration = 1495\n\nShrink model to first 1496 iterations.\ni ---> 4\n0:\tlearn: 1.1860533\ttest: 1.2303825\tbest: 1.2303825 (0)\ttotal: 127ms\tremaining: 12m 41s\n100:\tlearn: 0.1689237\ttest: 0.1743837\tbest: 0.1743478 (99)\ttotal: 11.4s\tremaining: 11m 4s\n200:\tlearn: 0.1436070\ttest: 0.1540533\tbest: 0.1539090 (198)\ttotal: 21.6s\tremaining: 10m 22s\n300:\tlearn: 0.1316892\ttest: 0.1470687\tbest: 0.1467859 (283)\ttotal: 31.6s\tremaining: 9m 57s\n400:\tlearn: 0.1242246\ttest: 0.1432192\tbest: 0.1431608 (396)\ttotal: 41.9s\tremaining: 9m 44s\n500:\tlearn: 0.1190122\ttest: 0.1414259\tbest: 0.1413681 (498)\ttotal: 52.1s\tremaining: 9m 32s\n600:\tlearn: 0.1144251\ttest: 0.1389905\tbest: 0.1389905 (600)\ttotal: 1m 2s\tremaining: 9m 22s\n700:\tlearn: 0.1104376\ttest: 0.1377376\tbest: 0.1377376 (700)\ttotal: 1m 13s\tremaining: 9m 13s\n800:\tlearn: 0.1066961\ttest: 0.1369497\tbest: 0.1368214 (774)\ttotal: 1m 23s\tremaining: 9m 4s\n900:\tlearn: 0.1031681\ttest: 0.1366318\tbest: 0.1365540 (897)\ttotal: 1m 34s\tremaining: 8m 53s\n1000:\tlearn: 0.1001683\ttest: 0.1361841\tbest: 0.1359366 (968)\ttotal: 1m 44s\tremaining: 8m 43s\n1100:\tlearn: 0.0971485\ttest: 0.1357743\tbest: 0.1357729 (1099)\ttotal: 1m 55s\tremaining: 8m 34s\n1200:\tlearn: 0.0944720\ttest: 0.1357360\tbest: 0.1356305 (1157)\ttotal: 2m 5s\tremaining: 8m 23s\n1300:\tlearn: 0.0918977\ttest: 0.1356840\tbest: 0.1355774 (1280)\ttotal: 2m 17s\tremaining: 8m 15s\n1400:\tlearn: 0.0895992\ttest: 0.1355448\tbest: 0.1354249 (1379)\ttotal: 2m 28s\tremaining: 8m 6s\n1500:\tlearn: 0.0874675\ttest: 0.1355165\tbest: 0.1354013 (1487)\ttotal: 2m 38s\tremaining: 7m 55s\n1600:\tlearn: 0.0851688\ttest: 0.1355677\tbest: 0.1353878 (1580)\ttotal: 2m 48s\tremaining: 7m 44s\n1700:\tlearn: 0.0831273\ttest: 0.1358237\tbest: 0.1353878 (1580)\ttotal: 2m 59s\tremaining: 7m 33s\nStopped by overfitting detector  (20 iterations wait)\n\nbestTest = 0.1353878356\nbestIteration = 1580\n\nShrink model to first 1581 iterations.\n\n---> 9809\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb327dd79da246c8b8566b471bcc1fcc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "i ---> 0\n[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002409 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2300\n[LightGBM] [Info] Number of data points in the train set: 7847, number of used features: 138\n[LightGBM] [Info] Start training from score 1191612.081050\n[LightGBM] [Warning]No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning]No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\ni ---> 1\n[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001660 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2307\n[LightGBM] [Info] Number of data points in the train set: 7847, number of used features: 140\n[LightGBM] [Info] Start training from score 1201057.729068\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning]No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\ni ---> 2\n[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003563 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2304\n[LightGBM] [Info] Number of data points in the train set: 7847, number of used features: 137\n[LightGBM] [Info] Start training from score 1192985.854467\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning]No further splits with positive gain, best gain: -inf\ni ---> 3\n[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002003 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2302\n[LightGBM] [Info] Number of data points in the train set: 7847, number of used features: 139\n[LightGBM] [Info] Start training from score 1199360.902256\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning]No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\ni ---> 4\n[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002834 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2308\n[LightGBM] [Info] Number of data points in the train set: 7848, number of used features: 141\n[LightGBM] [Info] Start training from score 1197112.003058\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning]No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n---> 9809\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f48ef9724aef4d8185145122f3353d3a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "i ---> 0\n[23:12:01] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { importance_type, missing, n_estimators } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[0]\ttrain-mape:0.89608\teval-mape:0.89486\nMultiple eval metrics have been passed: 'eval-mape' will be used for early stopping.\n\nWill train until eval-mape hasn't improved in 10 rounds.\n[1]\ttrain-mape:0.80278\teval-mape:0.80143\n[2]\ttrain-mape:0.71966\teval-mape:0.71844\n[3]\ttrain-mape:0.64480\teval-mape:0.64397\n[4]\ttrain-mape:0.57885\teval-mape:0.57809\n[5]\ttrain-mape:0.51941\teval-mape:0.51875\n[6]\ttrain-mape:0.46640\teval-mape:0.46544\n[7]\ttrain-mape:0.41923\teval-mape:0.41852\n[8]\ttrain-mape:0.37682\teval-mape:0.37599\n[9]\ttrain-mape:0.33914\teval-mape:0.33954\n[10]\ttrain-mape:0.30614\teval-mape:0.30813\n[11]\ttrain-mape:0.27737\teval-mape:0.28091\n[12]\ttrain-mape:0.25153\teval-mape:0.25690\n[13]\ttrain-mape:0.22887\teval-mape:0.23652\n[14]\ttrain-mape:0.20931\teval-mape:0.21918\n[15]\ttrain-mape:0.19252\teval-mape:0.20454\n[16]\ttrain-mape:0.17785\teval-mape:0.19151\n[17]\ttrain-mape:0.16532\teval-mape:0.18081\n[18]\ttrain-mape:0.15494\teval-mape:0.17209\n[19]\ttrain-mape:0.14612\teval-mape:0.16469\n[20]\ttrain-mape:0.13903\teval-mape:0.15900\n[21]\ttrain-mape:0.13265\teval-mape:0.15385\n[22]\ttrain-mape:0.12780\teval-mape:0.15002\n[23]\ttrain-mape:0.12334\teval-mape:0.14664\n[24]\ttrain-mape:0.12011\teval-mape:0.14434\n[25]\ttrain-mape:0.11709\teval-mape:0.14296\n[26]\ttrain-mape:0.11470\teval-mape:0.14144\n[27]\ttrain-mape:0.11262\teval-mape:0.14025\n[28]\ttrain-mape:0.11112\teval-mape:0.13952\n[29]\ttrain-mape:0.10996\teval-mape:0.13889\n[30]\ttrain-mape:0.10908\teval-mape:0.13866\n[31]\ttrain-mape:0.10825\teval-mape:0.13850\n[32]\ttrain-mape:0.10751\teval-mape:0.13857\n[33]\ttrain-mape:0.10697\teval-mape:0.13870\n[34]\ttrain-mape:0.10656\teval-mape:0.13874\n[35]\ttrain-mape:0.10598\teval-mape:0.13856\n[36]\ttrain-mape:0.10569\teval-mape:0.13869\n[37]\ttrain-mape:0.10538\teval-mape:0.13879\n[38]\ttrain-mape:0.10507\teval-mape:0.13883\n[39]\ttrain-mape:0.10468\teval-mape:0.13884\n[40]\ttrain-mape:0.10447\teval-mape:0.13898\n[41]\ttrain-mape:0.10435\teval-mape:0.13919\nStopping. Best iteration:\n[31]\ttrain-mape:0.10825\teval-mape:0.13850\n\ni ---> 1\n[23:12:02] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { importance_type, missing, n_estimators } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[0]\ttrain-mape:0.89667\teval-mape:0.89559\nMultiple eval metrics have been passed: 'eval-mape' will be used for early stopping.\n\nWill train until eval-mape hasn't improved in 10 rounds.\n[1]\ttrain-mape:0.80348\teval-mape:0.80262\n[2]\ttrain-mape:0.71959\teval-mape:0.71921\n[3]\ttrain-mape:0.64419\teval-mape:0.64373\n[4]\ttrain-mape:0.57757\teval-mape:0.57727\n[5]\ttrain-mape:0.51767\teval-mape:0.51815\n[6]\ttrain-mape:0.46454\teval-mape:0.46609\n[7]\ttrain-mape:0.41736\teval-mape:0.41978\n[8]\ttrain-mape:0.37458\teval-mape:0.37718\n[9]\ttrain-mape:0.33731\teval-mape:0.34065\n[10]\ttrain-mape:0.30373\teval-mape:0.30832\n[11]\ttrain-mape:0.27478\teval-mape:0.28113\n[12]\ttrain-mape:0.24921\teval-mape:0.25698\n[13]\ttrain-mape:0.22691\teval-mape:0.23632\n[14]\ttrain-mape:0.20767\teval-mape:0.21807\n[15]\ttrain-mape:0.19098\teval-mape:0.20247\n[16]\ttrain-mape:0.17668\teval-mape:0.18906\n[17]\ttrain-mape:0.16420\teval-mape:0.17810\n[18]\ttrain-mape:0.15399\teval-mape:0.16916\n[19]\ttrain-mape:0.14549\teval-mape:0.16153\n[20]\ttrain-mape:0.13814\teval-mape:0.15530\n[21]\ttrain-mape:0.13200\teval-mape:0.15046\n[22]\ttrain-mape:0.12672\teval-mape:0.14636\n[23]\ttrain-mape:0.12244\teval-mape:0.14366\n[24]\ttrain-mape:0.11901\teval-mape:0.14125\n[25]\ttrain-mape:0.11593\teval-mape:0.13993\n[26]\ttrain-mape:0.11369\teval-mape:0.13881\n[27]\ttrain-mape:0.11222\teval-mape:0.13813\n[28]\ttrain-mape:0.11065\teval-mape:0.13749\n[29]\ttrain-mape:0.10958\teval-mape:0.13706\n[30]\ttrain-mape:0.10884\teval-mape:0.13700\n[31]\ttrain-mape:0.10838\teval-mape:0.13687\n[32]\ttrain-mape:0.10775\teval-mape:0.13668\n[33]\ttrain-mape:0.10718\teval-mape:0.13681\n[34]\ttrain-mape:0.10649\teval-mape:0.13668\n[35]\ttrain-mape:0.10610\teval-mape:0.13689\n[36]\ttrain-mape:0.10556\teval-mape:0.13706\n[37]\ttrain-mape:0.10523\teval-mape:0.13723\n[38]\ttrain-mape:0.10525\teval-mape:0.13778\n[39]\ttrain-mape:0.10519\teval-mape:0.13809\n[40]\ttrain-mape:0.10491\teval-mape:0.13830\n[41]\ttrain-mape:0.10496\teval-mape:0.13848\n[42]\ttrain-mape:0.10505\teval-mape:0.13873\nStopping. Best iteration:\n[32]\ttrain-mape:0.10775\teval-mape:0.13668\n\ni ---> 2\n[23:12:04] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { importance_type, missing, n_estimators } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[0]\ttrain-mape:0.89603\teval-mape:0.89597\nMultiple eval metrics have been passed: 'eval-mape' will be used for early stopping.\n\nWill train until eval-mape hasn't improved in 10 rounds.\n[1]\ttrain-mape:0.80323\teval-mape:0.80202\n[2]\ttrain-mape:0.72001\teval-mape:0.71815\n[3]\ttrain-mape:0.64511\teval-mape:0.64316\n[4]\ttrain-mape:0.57869\teval-mape:0.57668\n[5]\ttrain-mape:0.51940\teval-mape:0.51741\n[6]\ttrain-mape:0.46628\teval-mape:0.46582\n[7]\ttrain-mape:0.41875\teval-mape:0.41925\n[8]\ttrain-mape:0.37572\teval-mape:0.37923\n[9]\ttrain-mape:0.33818\teval-mape:0.34368\n[10]\ttrain-mape:0.30499\teval-mape:0.31164\n[11]\ttrain-mape:0.27597\teval-mape:0.28450\n[12]\ttrain-mape:0.25014\teval-mape:0.26125\n[13]\ttrain-mape:0.22750\teval-mape:0.24177\n[14]\ttrain-mape:0.20787\teval-mape:0.22404\n[15]\ttrain-mape:0.19128\teval-mape:0.21046\n[16]\ttrain-mape:0.17654\teval-mape:0.19862\n[17]\ttrain-mape:0.16407\teval-mape:0.18829\n[18]\ttrain-mape:0.15354\teval-mape:0.17929\n[19]\ttrain-mape:0.14468\teval-mape:0.17257\n[20]\ttrain-mape:0.13731\teval-mape:0.16650\n[21]\ttrain-mape:0.13111\teval-mape:0.16117\n[22]\ttrain-mape:0.12595\teval-mape:0.15795\n[23]\ttrain-mape:0.12198\teval-mape:0.15537\n[24]\ttrain-mape:0.11875\teval-mape:0.15299\n[25]\ttrain-mape:0.11577\teval-mape:0.15180\n[26]\ttrain-mape:0.11341\teval-mape:0.15076\n[27]\ttrain-mape:0.11161\teval-mape:0.15035\n[28]\ttrain-mape:0.11021\teval-mape:0.14965\n[29]\ttrain-mape:0.10911\teval-mape:0.14909\n[30]\ttrain-mape:0.10854\teval-mape:0.14931\n[31]\ttrain-mape:0.10774\teval-mape:0.14928\n[32]\ttrain-mape:0.10732\teval-mape:0.14950\n[33]\ttrain-mape:0.10682\teval-mape:0.14949\n[34]\ttrain-mape:0.10626\teval-mape:0.14968\n[35]\ttrain-mape:0.10599\teval-mape:0.14984\n[36]\ttrain-mape:0.10554\teval-mape:0.14999\n[37]\ttrain-mape:0.10541\teval-mape:0.15023\n[38]\ttrain-mape:0.10512\teval-mape:0.15021\n[39]\ttrain-mape:0.10485\teval-mape:0.15005\nStopping. Best iteration:\n[29]\ttrain-mape:0.10911\teval-mape:0.14909\n\ni ---> 3\n[23:12:06] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { importance_type, missing, n_estimators } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[0]\ttrain-mape:0.89611\teval-mape:0.89590\nMultiple eval metrics have been passed: 'eval-mape' will be used for early stopping.\n\nWill train until eval-mape hasn't improved in 10 rounds.\n[1]\ttrain-mape:0.80329\teval-mape:0.80184\n[2]\ttrain-mape:0.71981\teval-mape:0.71766\n[3]\ttrain-mape:0.64429\teval-mape:0.64192\n[4]\ttrain-mape:0.57720\teval-mape:0.57436\n[5]\ttrain-mape:0.51784\teval-mape:0.51555\n[6]\ttrain-mape:0.46496\teval-mape:0.46347\n[7]\ttrain-mape:0.41802\teval-mape:0.41605\n[8]\ttrain-mape:0.37487\teval-mape:0.37353\n[9]\ttrain-mape:0.33785\teval-mape:0.33811\n[10]\ttrain-mape:0.30497\teval-mape:0.30700\n[11]\ttrain-mape:0.27618\teval-mape:0.27995\n[12]\ttrain-mape:0.25094\teval-mape:0.25641\n[13]\ttrain-mape:0.22835\teval-mape:0.23593\n[14]\ttrain-mape:0.20886\teval-mape:0.21835\n[15]\ttrain-mape:0.19172\teval-mape:0.20333\n[16]\ttrain-mape:0.17722\teval-mape:0.19111\n[17]\ttrain-mape:0.16466\teval-mape:0.18091\n[18]\ttrain-mape:0.15390\teval-mape:0.17216\n[19]\ttrain-mape:0.14492\teval-mape:0.16477\n[20]\ttrain-mape:0.13722\teval-mape:0.15882\n[21]\ttrain-mape:0.13090\teval-mape:0.15402\n[22]\ttrain-mape:0.12582\teval-mape:0.15033\n[23]\ttrain-mape:0.12189\teval-mape:0.14800\n[24]\ttrain-mape:0.11837\teval-mape:0.14611\n[25]\ttrain-mape:0.11534\teval-mape:0.14461\n[26]\ttrain-mape:0.11295\teval-mape:0.14312\n[27]\ttrain-mape:0.11103\teval-mape:0.14215\n[28]\ttrain-mape:0.10972\teval-mape:0.14154\n[29]\ttrain-mape:0.10811\teval-mape:0.14050\n[30]\ttrain-mape:0.10704\teval-mape:0.14003\n[31]\ttrain-mape:0.10610\teval-mape:0.13994\n[32]\ttrain-mape:0.10538\teval-mape:0.13988\n[33]\ttrain-mape:0.10497\teval-mape:0.13982\n[34]\ttrain-mape:0.10473\teval-mape:0.14007\n[35]\ttrain-mape:0.10427\teval-mape:0.14003\n[36]\ttrain-mape:0.10396\teval-mape:0.14036\n[37]\ttrain-mape:0.10353\teval-mape:0.14035\n[38]\ttrain-mape:0.10348\teval-mape:0.14072\n[39]\ttrain-mape:0.10352\teval-mape:0.14095\n[40]\ttrain-mape:0.10338\teval-mape:0.14115\n[41]\ttrain-mape:0.10325\teval-mape:0.14138\n[42]\ttrain-mape:0.10321\teval-mape:0.14154\n[43]\ttrain-mape:0.10317\teval-mape:0.14165\nStopping. Best iteration:\n[33]\ttrain-mape:0.10497\teval-mape:0.13982\n\ni ---> 4\n[23:12:08] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { importance_type, missing, n_estimators } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[0]\ttrain-mape:0.89614\teval-mape:0.89631\nMultiple eval metrics have been passed: 'eval-mape' will be used for early stopping.\n\nWill train until eval-mape hasn't improved in 10 rounds.\n[1]\ttrain-mape:0.80294\teval-mape:0.80353\n[2]\ttrain-mape:0.71937\teval-mape:0.72002\n[3]\ttrain-mape:0.64460\teval-mape:0.64471\n[4]\ttrain-mape:0.57714\teval-mape:0.57780\n[5]\ttrain-mape:0.51811\teval-mape:0.51905\n[6]\ttrain-mape:0.46486\teval-mape:0.46699\n[7]\ttrain-mape:0.41781\teval-mape:0.42089\n[8]\ttrain-mape:0.37585\teval-mape:0.37917\n[9]\ttrain-mape:0.33819\teval-mape:0.34277\n[10]\ttrain-mape:0.30527\teval-mape:0.31111\n[11]\ttrain-mape:0.27544\teval-mape:0.28295\n[12]\ttrain-mape:0.25030\teval-mape:0.25951\n[13]\ttrain-mape:0.22813\teval-mape:0.23923\n[14]\ttrain-mape:0.20869\teval-mape:0.22216\n[15]\ttrain-mape:0.19170\teval-mape:0.20714\n[16]\ttrain-mape:0.17722\teval-mape:0.19413\n[17]\ttrain-mape:0.16475\teval-mape:0.18331\n[18]\ttrain-mape:0.15441\teval-mape:0.17473\n[19]\ttrain-mape:0.14546\teval-mape:0.16748\n[20]\ttrain-mape:0.13825\teval-mape:0.16202\n[21]\ttrain-mape:0.13181\teval-mape:0.15756\n[22]\ttrain-mape:0.12660\teval-mape:0.15405\n[23]\ttrain-mape:0.12229\teval-mape:0.15105\n[24]\ttrain-mape:0.11865\teval-mape:0.14871\n[25]\ttrain-mape:0.11579\teval-mape:0.14714\n[26]\ttrain-mape:0.11335\teval-mape:0.14614\n[27]\ttrain-mape:0.11137\teval-mape:0.14517\n[28]\ttrain-mape:0.10990\teval-mape:0.14450\n[29]\ttrain-mape:0.10892\teval-mape:0.14398\n[30]\ttrain-mape:0.10786\teval-mape:0.14379\n[31]\ttrain-mape:0.10726\teval-mape:0.14394\n[32]\ttrain-mape:0.10654\teval-mape:0.14384\n[33]\ttrain-mape:0.10589\teval-mape:0.14392\n[34]\ttrain-mape:0.10549\teval-mape:0.14415\n[35]\ttrain-mape:0.10500\teval-mape:0.14420\n[36]\ttrain-mape:0.10446\teval-mape:0.14420\n[37]\ttrain-mape:0.10428\teval-mape:0.14422\n[38]\ttrain-mape:0.10398\teval-mape:0.14427\n[39]\ttrain-mape:0.10392\teval-mape:0.14461\n[40]\ttrain-mape:0.10389\teval-mape:0.14525\nStopping. Best iteration:\n[30]\ttrain-mape:0.10786\teval-mape:0.14379\n\n\nCG-CV: 0.13599414713680452\nXG-CV: 0.14297335370639183\nLG-CV: 0.1475786422972003\n------> (9809, 3)\n=============> (3837, 3)\n"
    }
   ],
   "source": [
    "x_train, labels, x_test = model_1(X, y, X_sub, cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[23:12:10] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[23:12:10] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[23:12:10] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[23:12:10] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[23:12:10] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[0]\ttrain-rmse:1604136.47500+6415.65624\ttest-rmse:1604000.97500+25632.60058\ttrain-mape:0.98959+0.00001\ttest-mape:0.98959+0.00005\n[10]\ttrain-rmse:1455530.37500+5826.72004\ttest-rmse:1455804.12500+23740.40109\ttrain-mape:0.89101+0.00005\ttest-mape:0.89093+0.00065\n[20]\ttrain-rmse:1321316.12500+5188.55894\ttest-rmse:1322097.10000+22330.15501\ttrain-mape:0.80196+0.00005\ttest-mape:0.80182+0.00107\n[30]\ttrain-rmse:1200204.02500+4643.55825\ttest-rmse:1201278.82500+20871.94343\ttrain-mape:0.72169+0.00008\ttest-mape:0.72152+0.00128\n[40]\ttrain-rmse:1090905.62500+4185.52582\ttest-rmse:1092418.67500+19594.54193\ttrain-mape:0.64946+0.00015\ttest-mape:0.64940+0.00139\n[50]\ttrain-rmse:992336.10000+3772.92358\ttest-rmse:994110.63750+18478.51389\ttrain-mape:0.58478+0.00031\ttest-mape:0.58480+0.00144\n[60]\ttrain-rmse:903488.07500+3456.46522\ttest-rmse:905639.02500+17471.42094\ttrain-mape:0.52707+0.00049\ttest-mape:0.52711+0.00160\n[70]\ttrain-rmse:823439.23750+3073.37034\ttest-rmse:826027.63750+16717.86524\ttrain-mape:0.47558+0.00065\ttest-mape:0.47572+0.00167\n[80]\ttrain-rmse:751476.15000+2833.42069\ttest-rmse:754492.12500+16014.10098\ttrain-mape:0.42960+0.00076\ttest-mape:0.42982+0.00167\n[90]\ttrain-rmse:686876.62500+2596.60383\ttest-rmse:690363.20000+15583.37371\ttrain-mape:0.38856+0.00085\ttest-mape:0.38887+0.00179\n[100]\ttrain-rmse:628881.57500+2440.60932\ttest-rmse:632850.83750+15231.81218\ttrain-mape:0.35209+0.00089\ttest-mape:0.35256+0.00194\n[110]\ttrain-rmse:576926.63750+2325.68674\ttest-rmse:581486.96250+14999.51741\ttrain-mape:0.31968+0.00095\ttest-mape:0.32024+0.00208\n[120]\ttrain-rmse:530471.26250+2268.29055\ttest-rmse:535677.18125+14895.06610\ttrain-mape:0.29095+0.00099\ttest-mape:0.29176+0.00228\n[130]\ttrain-rmse:489132.25625+2247.93093\ttest-rmse:495111.58125+14906.54984\ttrain-mape:0.26560+0.00100\ttest-mape:0.26674+0.00242\n[140]\ttrain-rmse:452328.28750+2343.70903\ttest-rmse:459045.68125+14871.46469\ttrain-mape:0.24340+0.00102\ttest-mape:0.24481+0.00248\n[150]\ttrain-rmse:419645.64375+2429.01517\ttest-rmse:427278.77500+14984.64439\ttrain-mape:0.22395+0.00102\ttest-mape:0.22564+0.00256\n[160]\ttrain-rmse:390774.31875+2597.29257\ttest-rmse:399363.81250+15170.50524\ttrain-mape:0.20711+0.00104\ttest-mape:0.20911+0.00262\n[170]\ttrain-rmse:365307.86875+2781.51234\ttest-rmse:374909.16875+15436.31497\ttrain-mape:0.19273+0.00101\ttest-mape:0.19507+0.00264\n[180]\ttrain-rmse:342930.60000+2987.39283\ttest-rmse:353555.87500+15698.38876\ttrain-mape:0.18056+0.00101\ttest-mape:0.18320+0.00263\n[190]\ttrain-rmse:323386.95000+3127.12495\ttest-rmse:335016.41250+16063.50903\ttrain-mape:0.17030+0.00097\ttest-mape:0.17328+0.00260\n[200]\ttrain-rmse:306264.60625+3329.77922\ttest-rmse:318957.33750+16377.62945\ttrain-mape:0.16168+0.00092\ttest-mape:0.16499+0.00263\n[210]\ttrain-rmse:291430.70625+3547.08409\ttest-rmse:305243.52500+16759.44343\ttrain-mape:0.15456+0.00090\ttest-mape:0.15820+0.00274\n[220]\ttrain-rmse:278581.67500+3718.41472\ttest-rmse:293478.62500+17028.13926\ttrain-mape:0.14876+0.00088\ttest-mape:0.15271+0.00286\n[230]\ttrain-rmse:267528.59375+3856.45940\ttest-rmse:283499.75000+17335.96283\ttrain-mape:0.14408+0.00086\ttest-mape:0.14832+0.00293\n[240]\ttrain-rmse:258021.98125+4022.00684\ttest-rmse:275036.18125+17611.19036\ttrain-mape:0.14032+0.00085\ttest-mape:0.14483+0.00298\n[250]\ttrain-rmse:249902.25000+4116.39057\ttest-rmse:267927.00312+17810.96556\ttrain-mape:0.13732+0.00083\ttest-mape:0.14209+0.00305\n[260]\ttrain-rmse:242949.60625+4238.83690\ttest-rmse:261990.92188+18025.75114\ttrain-mape:0.13496+0.00078\ttest-mape:0.13997+0.00314\n[270]\ttrain-rmse:237037.96563+4343.38913\ttest-rmse:257029.42188+18321.63369\ttrain-mape:0.13308+0.00076\ttest-mape:0.13831+0.00323\n[280]\ttrain-rmse:232021.60938+4429.94293\ttest-rmse:252900.51250+18537.07999\ttrain-mape:0.13169+0.00073\ttest-mape:0.13711+0.00329\n[290]\ttrain-rmse:227704.49375+4528.19886\ttest-rmse:249431.26250+18610.13006\ttrain-mape:0.13062+0.00070\ttest-mape:0.13624+0.00336\n[300]\ttrain-rmse:224027.71250+4599.15594\ttest-rmse:246598.40000+18736.83836\ttrain-mape:0.12983+0.00067\ttest-mape:0.13566+0.00344\n[310]\ttrain-rmse:220916.00937+4651.10205\ttest-rmse:244199.92188+18782.04189\ttrain-mape:0.12929+0.00063\ttest-mape:0.13527+0.00350\n[320]\ttrain-rmse:218287.25312+4691.25508\ttest-rmse:242251.18125+18799.82633\ttrain-mape:0.12891+0.00058\ttest-mape:0.13505+0.00359\n[330]\ttrain-rmse:216024.07187+4742.49003\ttest-rmse:240637.37188+18796.52382\ttrain-mape:0.12868+0.00056\ttest-mape:0.13496+0.00363\n[340]\ttrain-rmse:214093.73125+4750.56244\ttest-rmse:239323.32812+18793.55575\ttrain-mape:0.12859+0.00053\ttest-mape:0.13499+0.00367\n[350]\ttrain-rmse:212432.84063+4779.45822\ttest-rmse:238323.16562+18781.00528\ttrain-mape:0.12856+0.00049\ttest-mape:0.13509+0.00371\n\nEnsemble-CV: 215216.128125+/-4740.848765786417\n1----\n[23:12:17] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n2----\n3----\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1466544.4 , 2253873.5 , 1118813.4 , ...,  484047.28, 1040933.44,\n       1018714.25], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "preds = model_2()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1466544.4 , 2253873.5 , 1118813.4 , ...,  484047.28, 1040933.44,\n       1018714.25], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "count    3.837000e+03\nmean     1.479724e+06\nstd      1.170087e+06\nmin      1.305345e+05\n25%      7.385658e+05\n50%      1.114612e+06\n75%      1.789001e+06\nmax      8.722406e+06\ndtype: float64"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "pd.Series(preds).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id        price\n0   0  1466544.375\n1   1  2253873.500\n2   2  1118813.375\n3   3  2311837.250\n4   4  4568785.500\n5   5  1915298.250\n6   6  1013205.250\n7   7   622664.875\n8   8  1314327.750\n9   9  1319853.125",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1466544.375</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2253873.500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1118813.375</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2311837.250</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4568785.500</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>1915298.250</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>1013205.250</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>622664.875</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>1314327.750</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>1319853.125</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(DIR_TEST+'sample_submission.csv')\n",
    "sample_submission['price'] = preds\n",
    "sample_submission['price'] = sample_submission['price'].astype('float')\n",
    "sample_submission.to_csv(f'fin_stack_submission_blend_v{VERSION}.csv', index=False)\n",
    "sample_submission.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda736513bb9b354aa1af050f7f1a86d0f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}